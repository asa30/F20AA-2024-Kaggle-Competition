{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c9587e",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fgs2/f20aa-2024/blob/main/cw2/tfidfCopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa143d9-d6fb-4645-90d4-931efcf50690",
   "metadata": {
    "id": "eaa143d9-d6fb-4645-90d4-931efcf50690"
   },
   "source": [
    "# F20AA Applied Text Analytics: Coursework 2 - TF-IDF Notebook\n",
    "#### Deadline: 11:59pm, Monday 1st April 2024 via Canvas group space\n",
    "\n",
    "#### Members:\n",
    "- Francis Sandrino (fgs2)\n",
    "- Jai Varsani (jv81)\n",
    "- Ahmed Moussa Abdelfattah (asa30)\n",
    "- Aamir Nazir (mn2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53bb83-251f-4ba9-ae90-6cd1cac5f8a7",
   "metadata": {
    "id": "1d53bb83-251f-4ba9-ae90-6cd1cac5f8a7"
   },
   "source": [
    "### What is this?\n",
    "The purpose of this notebook is to serve as a form of parallelization with different Google Colab accounts to speed up experimentation. This notebook will have minimal documentation, only to aid the group members in understanding the code. The proper documentation, results, and discussion for all processing notebooks is included in the [main file](../amazonCW.ipynb).\n",
    "\n",
    "### What does this specific notebook deal with?\n",
    "Experimentation with TF-IDF.\n",
    "\n",
    "### TODO: Experimental Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iaodp_ZI7SrE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaodp_ZI7SrE",
    "outputId": "569237f5-fd84-4adc-acab-59a905a94283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'trainLemmatized.csv' downloaded successfully.\n",
      "File 'trainStemmed.csv' downloaded successfully.\n",
      "File 'testLemmatized.csv' downloaded successfully.\n",
      "File 'testStemmed.csv' downloaded successfully.\n",
      "File 'lemmaTokenizer.json' downloaded successfully.\n",
      "File 'stemTokenizer.json' downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# This is so I don't have to keep uploading on Colab.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "def downloadFileFromRepo(username, repository, branch, filepath, token):\n",
    "    # Construct the URL to download the file from GitHub\n",
    "    url = f\"https://raw.githubusercontent.com/{username}/{repository}/{branch}/{filepath}\"\n",
    "\n",
    "    # Send a GET request to download the file\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(username, token))\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the file name from the URL\n",
    "        fileName = filepath.split('/')[-1]\n",
    "\n",
    "        # Create the 'data' directory if it doesn't exist\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "\n",
    "        # Define the file path within the 'data' directory\n",
    "        localFilepath = os.path.join('data', fileName)\n",
    "\n",
    "        # Write the file content to a local file\n",
    "        with open(localFilepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File '{fileName}' downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "username = \"\"\n",
    "repository = \"\"\n",
    "branch = \"\"\n",
    "path_to_file = \"\"\n",
    "repoToken = \"\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/trainStemmed.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/testLemmatized.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/testStemmed.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/lemmaTokenizer.json\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/stemTokenizer.json\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615006b0-1ec7-47eb-a2b6-97708937bb37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "615006b0-1ec7-47eb-a2b6-97708937bb37",
    "outputId": "002e41dc-fe55-4f9c-d3cc-eba2df4e5b0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.1.2\n",
      "    Uninstalling pip-23.1.2:\n",
      "      Successfully uninstalled pip-23.1.2\n",
      "Successfully installed pip-24.0\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow\n",
    "!pip install pyyaml h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "seed = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6HCk5RWooBhx",
   "metadata": {
    "id": "6HCk5RWooBhx"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/trainLemmatized.csv\")\n",
    "training, testing = train_test_split(dataset, stratify = dataset['labels'], test_size=0.2, random_state=42)\n",
    "training.to_csv(\"data/trainLemmatizedTr.csv\", index = False)\n",
    "testing.to_csv(\"data/trainLemmatizedTe.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b61715-79cb-4c62-8f05-592b9a78d370",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9b61715-79cb-4c62-8f05-592b9a78d370",
    "outputId": "e315c474-2a37-4efa-ef94-d0a650bbe344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokenizer loaded successfully with 77413 words.\n",
      "Stemmed Tokenizer loaded successfully with 64940 words.\n"
     ]
    }
   ],
   "source": [
    "# Empirical value\n",
    "MAXLENGTH = 1885\n",
    "\n",
    "# Determines number of rows per batch to process on\n",
    "trainBatchSize = 512\n",
    "\n",
    "lemmatizedDataset = tf.data.experimental.make_csv_dataset(\"data/trainLemmatizedTr.csv\",\n",
    "                                                batch_size = trainBatchSize,\n",
    "                                                select_columns = [\"data\", \"labels\"],\n",
    "                                                label_name = \"labels\",\n",
    "                                                num_epochs = 10,\n",
    "                                                shuffle_seed = 43,\n",
    "                                                shuffle = True)\n",
    "\n",
    "# Loading tokenizers from the JSON files\n",
    "with open(\"data/lemmaTokenizer.json\", \"r\") as json_file:\n",
    "    tokenizerJSON = json_file.read()\n",
    "    lemmaTokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizerJSON)\n",
    "lemmaVocabSize = len(lemmaTokenizer.word_index)\n",
    "print(f\"Lemmatized Tokenizer loaded successfully with {lemmaVocabSize} words.\")\n",
    "\n",
    "# Loading tokenizers from the JSON files\n",
    "with open(\"data/stemTokenizer.json\", \"r\") as json_file:\n",
    "    tokenizerJSON = json_file.read()\n",
    "    stemTokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizerJSON)\n",
    "stemVocabSize = len(stemTokenizer.word_index)\n",
    "print(f\"Stemmed Tokenizer loaded successfully with {stemVocabSize} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53p8FgLiq8Hg",
   "metadata": {
    "id": "53p8FgLiq8Hg"
   },
   "outputs": [],
   "source": [
    "toTest = pd.read_csv(\"data/trainLemmatizedTe.csv\")\n",
    "toTestData = toTest['data'].tolist()\n",
    "toTestLabels = toTest['labels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7Cu4KI2Yxbce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "7Cu4KI2Yxbce",
    "outputId": "250a2e92-eafa-403d-f5e2-401d2f1800b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 1\n",
      "Next accuracy update at batch: 200\n",
      "Max accuracy: 0\n",
      "Latest accuracy: 0\n",
      "Average accuracy: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Iterator to avoid loading the entire dataset\n",
    "iterator = iter(lemmatizedDataset)\n",
    "# To keep track of which batch we're operating on\n",
    "progress = 0\n",
    "updateAccuracy = 200\n",
    "# Folds for cross-validation, uncomment when necessary\n",
    "# kSplits = 10\n",
    "\n",
    "# For evaluation later\n",
    "losses = []\n",
    "accuracies = []\n",
    "# holdoutA = 0\n",
    "# holdoutL = 0\n",
    "aveAccuracy = 0\n",
    "maxAccuracy = 0\n",
    "accuracy = 0\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # Embedding(lemmaVocabSize, 128, input_length = MAXLENGTH),\n",
    "    # Dropout(0.2),\n",
    "    # Bidirectional(LSTM(128)),\n",
    "    # Dropout(0.2),\n",
    "    # Dense(64, activation='relu'),\n",
    "    # Dense(5, activation='softmax')\n",
    "\n",
    "    Embedding(lemmaVocabSize, 128, input_length = MAXLENGTH),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(Embedding(vocabSize, 64, input_length = maxLength))\n",
    "# model.add(Bidirectional(LSTM(100)))\n",
    "#     # Dense(1024, input_shape=(77413,), activation='relu'),\n",
    "# model.add(tf.keras.layers.Dropout(0.2))\n",
    "#     # Dense(512, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.5),\n",
    "#     # Dense(256, activation='relu'),\n",
    "#     # tf.keras.layers.Dropout(0.5),\n",
    "# model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "toTestDataList = [str(item) for item in toTestData]\n",
    "tokenizedTestingData = lemmaTokenizer.texts_to_sequences(toTestDataList)\n",
    "paddedTestingData = tf.keras.preprocessing.sequence.pad_sequences(tokenizedTestingData, maxlen=MAXLENGTH, padding=\"post\")\n",
    "encodedToTestLabels = [x - 1 for x in toTestLabels]\n",
    "encodedTestingLabels = tf.keras.utils.to_categorical(encodedToTestLabels, num_classes = 5)\n",
    "\n",
    "try:\n",
    "  while True:\n",
    "    # Admin stuff\n",
    "    progress = progress + 1\n",
    "    if progress % 200 == 0:\n",
    "      updateAccuracy = progress\n",
    "    clear_output(wait = True)\n",
    "    print(f\"Batch number: {progress}\")\n",
    "    # print(f\"Last holdout accuracy: {holdoutA}\")\n",
    "    print(f\"Next accuracy update at batch: {updateAccuracy}\")\n",
    "    print(f\"Max accuracy: {maxAccuracy}\")\n",
    "    print(f\"Latest accuracy: {accuracy}\")\n",
    "    print(f\"Average accuracy: {aveAccuracy}\")\n",
    "\n",
    "    # Obtain batch of text as a list\n",
    "    batch = next(iterator)\n",
    "    current = batch[0]['data'].numpy().tolist()\n",
    "    decoded = list(map((lambda x : x.decode()), current))\n",
    "\n",
    "    # Keep track of labels of each batch\n",
    "    currentLabels = batch[1].numpy().tolist()\n",
    "\n",
    "    # Convert to matrix of binaries (1 if the word occurs, 0 otherwise)\n",
    "    # tokenizedDocs = loadedTokenizer.texts_to_matrix(decoded, mode = 'tfidf')\n",
    "    tokenizedDocs = lemmaTokenizer.texts_to_sequences(decoded)\n",
    "    paddedData=tf.keras.preprocessing.sequence.pad_sequences(tokenizedDocs, maxlen=MAXLENGTH, padding=\"post\")\n",
    "\n",
    "    adjustedLabels = [x - 1 for x in currentLabels]\n",
    "    ohEncodedLabels = tf.keras.utils.to_categorical(adjustedLabels, num_classes = 5)\n",
    "\n",
    "\n",
    "    # skf = StratifiedKFold(n_splits = kSplits, shuffle = True, random_state = seed)\n",
    "    # # 10-fold cross-validation\n",
    "    # for trainIndex, testIndex in skf.split(xTrain, yTrain):\n",
    "\n",
    "    #   xTrainFold, xTestFold = xTrain[trainIndex], xTrain[testIndex]\n",
    "    #   yTrainFold, yTestFold = [yTrain[i] - 1 for i in trainIndex], [yTrain[i] - 1 for i in testIndex]\n",
    "\n",
    "    #   yTrainEncoded = tf.keras.utils.to_categorical(yTrainFold, num_classes = 5)\n",
    "    #   yTestEncoded = tf.keras.utils.to_categorical(yTestFold, num_classes = 5)\n",
    "\n",
    "    #   model.train_on_batch(xTrainFold, yTrainEncoded)\n",
    "\n",
    "    #   loss, accuracy = model.evaluate(xTestFold, yTestEncoded)\n",
    "    #   losses.append(loss)\n",
    "    #   accuracies.append(accuracy)\n",
    "    model.train_on_batch(paddedData, ohEncodedLabels)\n",
    "    if progress % 200 == 0:\n",
    "\n",
    "      loss, accuracy = model.evaluate(paddedTestingData, encodedTestingLabels)\n",
    "      if maxAccuracy < accuracy:\n",
    "        maxAccuracy = accuracy\n",
    "      aveAccuracy = (((aveAccuracy * (progress - 1)) + accuracy) / updateAccuracy)\n",
    "      updateAccuracy = updateAccuracy + 200\n",
    "\n",
    "    # print(f\"Validation Loss: {loss}\")\n",
    "    # print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "    # Compute average validation metrics\n",
    "    # avgLoss = np.mean(losses)\n",
    "    # avgAcc = np.mean(accuracies)\n",
    "\n",
    "    # print(f\"Average Validation Loss: {avgLoss}\")\n",
    "    # print(f\"Average Validation Accuracy: {avgAcc}\")\n",
    "\n",
    "    # losses = []\n",
    "    # accuracies = []\n",
    "\n",
    "    # Evaluate the model on holdout set\n",
    "    # yTest = [x - 1 for x in yTest]\n",
    "    # yTestEncoded = tf.keras.utils.to_categorical(yTest, num_classes=5)\n",
    "\n",
    "    # holdoutL, holdoutA = model.evaluate(xTest, yTestEncoded)\n",
    "    # print(f\"Holdout Loss: {holdoutL}, Holdout Accuracy: {holdoutA}\")\n",
    "\n",
    "except StopIteration:\n",
    "  print(\"End of iterator reached.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CrtQNcxfwPw8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrtQNcxfwPw8",
    "outputId": "4eb5a8e1-1f56-4fe4-b6c8-a6a6363ba79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch number: 95\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "testBatchSize = 256\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\"data/testLemmatized.csv\",\n",
    "                                                batch_size = testBatchSize,\n",
    "                                                select_columns = [\"processed\"],\n",
    "                                                num_epochs = 1,\n",
    "                                                shuffle = False)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "preds = []\n",
    "lemmatizedDocs = []\n",
    "progress = 0\n",
    "\n",
    "try:\n",
    "  while True:\n",
    "\n",
    "    # Admin stuff\n",
    "    progress = progress + 1\n",
    "    clear_output(wait = True)\n",
    "    print(f\"Test batch number: {progress}\")\n",
    "\n",
    "    # Obtain batch of text as a list\n",
    "    batch = next(iterator)\n",
    "    current = batch['processed'].numpy().tolist()\n",
    "    decoded = list(map((lambda x : x.decode()), current))\n",
    "\n",
    "    # Proper model input format\n",
    "    tokenizedDocs = lemmaTokenizer.texts_to_sequences(decoded)\n",
    "    paddedData=tf.keras.preprocessing.sequence.pad_sequences(tokenizedDocs, maxlen=MAXLENGTH, padding=\"post\")\n",
    "\n",
    "    # Testing against the model\n",
    "    pred = model.predict(paddedData)\n",
    "    truePredict = np.argmax(pred, axis = 1) + 1\n",
    "    preds.append(truePredict)\n",
    "\n",
    "except StopIteration:\n",
    "  print(\"End of iterator reached.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qNQRl3q5C9h_",
   "metadata": {
    "id": "qNQRl3q5C9h_"
   },
   "outputs": [],
   "source": [
    "concatenated = np.concatenate(preds)\n",
    "results = pd.DataFrame(concatenated)\n",
    "results.rename(columns = {0 : 'overall'}, inplace = True)\n",
    "results.insert(0, 'id', range(len(results)))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yrC9nSD-GJAO",
   "metadata": {
    "id": "yrC9nSD-GJAO"
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"SimpleCNN1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J2Ljn98M-8e6",
   "metadata": {
    "id": "J2Ljn98M-8e6"
   },
   "outputs": [],
   "source": [
    "model.save('SimpleCNN1.keras')\n",
    "# loaded_model = tf.keras.saving.load_model('insert-model-name.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
