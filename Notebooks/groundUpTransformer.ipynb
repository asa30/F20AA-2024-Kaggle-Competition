{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84a70c8",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fgs2/f20aa-2024/blob/main/cw2/transformers/groundUpTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa143d9-d6fb-4645-90d4-931efcf50690",
   "metadata": {
    "id": "eaa143d9-d6fb-4645-90d4-931efcf50690"
   },
   "source": [
    "# F20AA Applied Text Analytics: Coursework 2 - Custom Transformer Notebook\n",
    "#### Deadline: 11:59pm, Monday 1st April 2024 via Canvas group space\n",
    "\n",
    "#### Members:\n",
    "- Francis Sandrino (fgs2)\n",
    "- Jai Varsani (jv81)\n",
    "- Ahmed Moussa Abdelfattah (asa30)\n",
    "- Aamir Nazir (mn2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53bb83-251f-4ba9-ae90-6cd1cac5f8a7",
   "metadata": {
    "id": "1d53bb83-251f-4ba9-ae90-6cd1cac5f8a7"
   },
   "source": [
    "### What is this?\n",
    "The purpose of this notebook is to serve as a form of parallelization with different Google Colab accounts to speed up experimentation. This notebook will have minimal documentation, only to aid the group members in understanding the code. The proper documentation, results, and discussion for all processing notebooks is included in the [main file](../amazonCW.ipynb).\n",
    "\n",
    "### What does this specific notebook deal with?\n",
    "Creating a transformer from the ground up and experimenting with it.\n",
    "\n",
    "### TODO: Experimental Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "iaodp_ZI7SrE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaodp_ZI7SrE",
    "outputId": "6bb3eb3c-f9c8-4850-f00c-dc5d2cfd4f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'trainLemmatized.csv' downloaded successfully.\n",
      "File 'trainStemmed.csv' downloaded successfully.\n",
      "File 'testLemmatized.csv' downloaded successfully.\n",
      "File 'testStemmed.csv' downloaded successfully.\n",
      "File 'lemmaTokenizer.json' downloaded successfully.\n",
      "File 'stemTokenizer.json' downloaded successfully.\n",
      "File 'train.csv' downloaded successfully.\n",
      "File 'test.csv' downloaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# This is so I don't have to keep uploading on Colab.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "def downloadFileFromRepo(username, repository, branch, filepath, token):\n",
    "    # Construct the URL to download the file from GitHub\n",
    "    url = f\"https://raw.githubusercontent.com/{username}/{repository}/{branch}/{filepath}\"\n",
    "\n",
    "    # Send a GET request to download the file\n",
    "    response = requests.get(url, auth=HTTPBasicAuth(username, token))\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Extract the file name from the URL\n",
    "        fileName = filepath.split('/')[-1]\n",
    "\n",
    "        # Create the 'data' directory if it doesn't exist\n",
    "        if not os.path.exists('data'):\n",
    "            os.makedirs('data')\n",
    "\n",
    "        # Define the file path within the 'data' directory\n",
    "        localFilepath = os.path.join('data', fileName)\n",
    "\n",
    "        # Write the file content to a local file\n",
    "        with open(localFilepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"File '{fileName}' downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "username = \"\"\n",
    "repository = \"\"\n",
    "branch = \"\"\n",
    "path_to_file = \"\"\n",
    "repoToken = \"\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/trainStemmed.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/testLemmatized.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/testStemmed.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/lemmaTokenizer.json\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/stemTokenizer.json\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/train.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n",
    "\n",
    "path_to_file = \"cw2/data/test.csv\"\n",
    "downloadFileFromRepo(username, repository, branch, path_to_file, repoToken)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615006b0-1ec7-47eb-a2b6-97708937bb37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "615006b0-1ec7-47eb-a2b6-97708937bb37",
    "outputId": "3b0bb4e1-59d1-46f0-c8f1-c7cd2b00a649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade torch\n",
    "!pip install tensorflow\n",
    "!pip install pyyaml h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "frzOGE6p5fy0",
   "metadata": {
    "id": "frzOGE6p5fy0"
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
    "            query=x, key=x, value=x, embed_dim_to_check=x.size(-1),\n",
    "            num_heads=self.self_attn.num_heads,\n",
    "            in_proj_weight=self.self_attn.in_proj_weight,\n",
    "            in_proj_bias=self.self_attn.in_proj_bias,\n",
    "            bias_k=self.self_attn.bias_k,\n",
    "            bias_v=self.self_attn.bias_v,\n",
    "            add_zero_attn=self.self_attn.add_zero_attn,\n",
    "            dropout_p=self.self_attn.dropout,\n",
    "            out_proj_weight=self.self_attn.out_proj.weight,\n",
    "            out_proj_bias=self.self_attn.out_proj.bias,\n",
    "            training=True,  # Set training to True\n",
    "            key_padding_mask=mask,\n",
    "            need_weights=False,\n",
    "            attn_mask=None,\n",
    "            use_separate_proj_weight=False,\n",
    "            q_proj_weight=None,\n",
    "            k_proj_weight=None,\n",
    "            v_proj_weight=None,\n",
    "            static_k=None,\n",
    "            static_v=None,\n",
    "            average_attn_weights=False,\n",
    "            is_causal=False)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, num_classes, dropout):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.positional_encoding(self.embedding(x)))\n",
    "\n",
    "        enc_output = embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, None)\n",
    "\n",
    "        # Global average pooling\n",
    "        pooled_output = enc_output.mean(dim=1)\n",
    "\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "U49uoDMT6E3w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U49uoDMT6E3w",
    "outputId": "6e554bf0-bd7e-4bad-f191-05b6aecbc89e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 for processing\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Transformer model\n",
    "vocab_size = 77414  # Example vocabulary size\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 1024\n",
    "max_seq_length = 1885\n",
    "num_classes = 5  # Example number of classes\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerClassifier(vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, num_classes, dropout)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} for processing\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6HCk5RWooBhx",
   "metadata": {
    "id": "6HCk5RWooBhx"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/trainLemmatized.csv\")\n",
    "training, testing = train_test_split(dataset, stratify = dataset['labels'], test_size=0.1, random_state=42)\n",
    "training.to_csv(\"data/trainLemmatizedTr.csv\", index = False)\n",
    "testing.to_csv(\"data/trainLemmatizedTe.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b61715-79cb-4c62-8f05-592b9a78d370",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9b61715-79cb-4c62-8f05-592b9a78d370",
    "outputId": "02b511e0-9af4-4f8c-ec23-86ff0cbc6485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokenizer loaded successfully with 77413 words.\n"
     ]
    }
   ],
   "source": [
    "# # Empirical value\n",
    "# MAXLENGTH = 1885\n",
    "\n",
    "# # Loading stemmed tokenizer from the JSON file\n",
    "# with open(\"data/stemTokenizer.json\", \"r\") as json_file:\n",
    "#     tokenizerJSON = json_file.read()\n",
    "#     stemTokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizerJSON)\n",
    "# stemVocabSize = len(stemTokenizer.word_index)\n",
    "# print(f\"Stemmed Tokenizer loaded successfully with {stemVocabSize} words.\")\n",
    "\n",
    "# Determines number of rows per batch to process on\n",
    "trainBatchSize = 32\n",
    "\n",
    "lemmatizedDataset = tf.data.experimental.make_csv_dataset(\"data/trainLemmatizedTr.csv\",\n",
    "                                                batch_size = trainBatchSize,\n",
    "                                                select_columns = [\"data\", \"labels\"],\n",
    "                                                label_name = \"labels\",\n",
    "                                                num_epochs = 5,\n",
    "                                                shuffle_seed = 43,\n",
    "                                                shuffle = True)\n",
    "\n",
    "# Loading tokenizers from the JSON files\n",
    "with open(\"data/lemmaTokenizer.json\", \"r\") as json_file:\n",
    "    tokenizerJSON = json_file.read()\n",
    "    lemmaTokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizerJSON)\n",
    "lemmaVocabSize = len(lemmaTokenizer.word_index)\n",
    "print(f\"Lemmatized Tokenizer loaded successfully with {lemmaVocabSize} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ef8q_Y_LAkaN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ef8q_Y_LAkaN",
    "outputId": "a937f251-a307-4918-b279-4cd176d5a989"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 9048\n",
      "Batch loss: 0.3726446330547333\n",
      "Batch accuracy: 0.9375\n",
      "\n",
      "Next test accuracy update at batch: 10000\n",
      "Max accuracy: 0.7634832370566323\n",
      "Latest accuracy: 0.7634832370566323\n",
      "Average accuracy: 0.7634832370566323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "!mkdir models\n",
    "\n",
    "# Iterator to avoid loading the entire dataset\n",
    "iterator = iter(lemmatizedDataset)\n",
    "\n",
    "# To keep track of which batch we're operating on\n",
    "TESTINTERVAL = 5000\n",
    "accuracy = 0\n",
    "progress = 0\n",
    "updateAccuracy = 5000\n",
    "maxAccuracy = 0\n",
    "aveAccuracy = 0\n",
    "batch_loss = 0\n",
    "batch_accuracy = 0\n",
    "totalLoss = 0\n",
    "totalCorrect = 0\n",
    "totalSamples = 0\n",
    "\n",
    "dataset = pd.read_csv(\"data/trainLemmatizedTe.csv\")\n",
    "training = dataset['data'].tolist()\n",
    "testing = dataset['labels'].tolist()\n",
    "\n",
    "training = [str(item) for item in training]\n",
    "training = lemmaTokenizer.texts_to_sequences(training)\n",
    "training = tf.keras.preprocessing.sequence.pad_sequences(training, maxlen=max_seq_length, padding=\"post\")\n",
    "\n",
    "testing = [x - 1 for x in testing]\n",
    "testing = tf.keras.utils.to_categorical(testing, num_classes = 5)\n",
    "\n",
    "training = torch.tensor(training)\n",
    "training = training.to(device)\n",
    "testing = torch.tensor(testing)\n",
    "testing = testing.to(device)\n",
    "toTest = TensorDataset(training, testing)\n",
    "dataset = DataLoader(toTest, batch_size=trainBatchSize, shuffle=False)\n",
    "\n",
    "try:\n",
    "  while True:\n",
    "    # Admin stuff\n",
    "    model.train()\n",
    "    progress = progress + 1\n",
    "    if progress % TESTINTERVAL == 0:\n",
    "      updateAccuracy = progress\n",
    "    clear_output(wait = True)\n",
    "    print(f\"Batch number: {progress}\")\n",
    "    print(f\"Batch loss: {batch_loss}\")\n",
    "    print(f\"Batch accuracy: {batch_accuracy}\\n\")\n",
    "    print(f\"Next test accuracy update at batch: {updateAccuracy}\")\n",
    "    print(f\"Max accuracy: {maxAccuracy}\")\n",
    "    print(f\"Latest accuracy: {accuracy}\")\n",
    "    print(f\"Average accuracy: {aveAccuracy}\")\n",
    "\n",
    "\n",
    "    # Obtain batch of text as a list\n",
    "    batch = next(iterator)\n",
    "    current = batch[0]['data'].numpy().tolist()\n",
    "    decoded = list(map((lambda x : x.decode()), current))\n",
    "\n",
    "    # Keep track of labels of each batch\n",
    "    currentLabels = batch[1].numpy().tolist()\n",
    "\n",
    "    tokenizedDocs = lemmaTokenizer.texts_to_sequences(decoded)\n",
    "    paddedData = tf.keras.preprocessing.sequence.pad_sequences(tokenizedDocs, maxlen=1885, padding=\"post\")\n",
    "\n",
    "    currentLabels = [x - 1 for x in currentLabels]\n",
    "    currentLabels = tf.keras.utils.to_categorical(currentLabels, num_classes = 5)\n",
    "\n",
    "    padded_data_tensor = torch.tensor(paddedData)\n",
    "    current_labels_tensor = torch.tensor(currentLabels)\n",
    "    train_dataset = TensorDataset(padded_data_tensor, current_labels_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=trainBatchSize, shuffle=True)\n",
    "\n",
    "    totalLoss = 0\n",
    "    totalCorrect = 0\n",
    "    totalSamples = 0\n",
    "\n",
    "    for batch_inputs, batch_labels in train_dataloader:\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        # print(batch_inputs)\n",
    "        # print(f\"Length of inputs: {len(batch_inputs)}\")\n",
    "        # print(batch_labels)\n",
    "        # print(f\"Length of labels: {len(batch_labels)}\")\n",
    "        outputs = model(batch_inputs)  # Forward pass\n",
    "        # loss = criterion(outputs.view(-1, num_classes), batch_labels)\n",
    "        loss = criterion(outputs, batch_labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update parameters\n",
    "        totalLoss += loss.item() * batch_inputs.size(0)\n",
    "        actual = batch_labels.argmax(dim = 1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        totalCorrect += (predicted == actual).sum().item()\n",
    "        totalSamples += batch_inputs.size(0)\n",
    "\n",
    "    batch_loss = totalLoss / len(train_dataset)\n",
    "    batch_accuracy = totalCorrect / totalSamples\n",
    "\n",
    "    if progress % TESTINTERVAL == 0:\n",
    "      val_loss = 0\n",
    "      val_correct = 0\n",
    "      val_samples = 0\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        print()\n",
    "        print(\"Saving model...\")\n",
    "        torch.save(model.state_dict(), f\"models/groundUpTransformerB{progress}.pth\")\n",
    "        print(f\"Model saved under: models/groundUpTransformerB{progress}.pth\")\n",
    "        for i, (batch_inputs, batch_labels) in enumerate(dataset):\n",
    "            if i % 100 == 0:\n",
    "              print(f\"Testing batch {i} to {i + 100}\")\n",
    "            batch_inputs = batch_inputs.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            outputs = model(batch_inputs)\n",
    "            val_loss += criterion(outputs, batch_labels).item() * batch_inputs.size(0)\n",
    "            actual = batch_labels.argmax(dim = 1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_correct += (predicted == actual).sum().item()\n",
    "            val_samples += batch_inputs.size(0)\n",
    "        accuracy = val_correct / val_samples\n",
    "      if maxAccuracy < accuracy:\n",
    "        maxAccuracy = accuracy\n",
    "      aveAccuracy = (((aveAccuracy * (progress - 1)) + accuracy) / (updateAccuracy / TESTINTERVAL))\n",
    "      updateAccuracy = updateAccuracy + TESTINTERVAL\n",
    "\n",
    "except StopIteration:\n",
    "  print(\"End of iterator reached.\")\n",
    "\n",
    "# # Split data into training and validation sets\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(padded_texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# batch_size = 64\n",
    "\n",
    "# # Create TensorFlow Dataset\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "# train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))\n",
    "# val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "# testDataset = tf.data.Dataset.from_tensor_slices(val_texts)\n",
    "# testDataset = testDataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qNQRl3q5C9h_",
   "metadata": {
    "id": "qNQRl3q5C9h_"
   },
   "outputs": [],
   "source": [
    "concatenated = np.concatenate(preds)\n",
    "results = pd.DataFrame(concatenated)\n",
    "results.rename(columns = {0 : 'overall'}, inplace = True)\n",
    "results.insert(0, 'id', range(len(results)))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yrC9nSD-GJAO",
   "metadata": {
    "id": "yrC9nSD-GJAO"
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"SimpleCNN1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J2Ljn98M-8e6",
   "metadata": {
    "id": "J2Ljn98M-8e6"
   },
   "outputs": [],
   "source": [
    "model.save('SimpleCNN1.keras')\n",
    "# loaded_model = tf.keras.saving.load_model('insert-model-name.keras')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
